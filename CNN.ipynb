{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import nn\n",
    "from tensorflow import layers\n",
    "\n",
    "def cnn_net(train_data, V, W, w_bias):    \n",
    "    # convolutional layer 1\n",
    "    conv1=tf.layers.conv2d(train_data, filters=32,kernel_size=[5,5], padding='VALID', activation=tf.nn.relu)\n",
    "    conv1=tf.layers.max_pooling2d(conv1, pool_size=[2,2], \n",
    "                             strides=2, padding='VALID')\n",
    "    \n",
    "    # convolutional layer 2\n",
    "    conv2=tf.layers.conv2d(conv1, filters=64,kernel_size=[5,5], padding='VALID', activation=tf.nn.relu)\n",
    "    conv2=tf.layers.max_pooling2d(conv2, pool_size=[2,2], \n",
    "                             strides=2, padding='VALID')\n",
    "    \n",
    "    # resize\n",
    "    conv3=tf.transpose(tf.reshape(conv2,[-1,4*4*64]))\n",
    "\n",
    "    # Hidden units\n",
    "    h=tf.nn.relu(tf.matmul(V,conv3))\n",
    "\n",
    "    # Output units\n",
    "    z_pre_soft=tf.matmul(W,h)+w_bias\n",
    "    z=tf.nn.softmax(z_pre_soft)\n",
    "    z=tf.sigmoid(z_pre_soft)\n",
    "\n",
    "    return z_pre_soft, z\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import nn\n",
    "import random\n",
    "\n",
    "# constants\n",
    "# change window size\n",
    "# check what to do if length<max_num_words\n",
    "max_iter=10000\n",
    "max_num_words=28\n",
    "num_categ=10\n",
    "input_vec_size=28\n",
    "tot_input_vec_size=input_vec_size+1\n",
    "num_hid_units=400\n",
    "epsw=tf.constant(0.002,tf.float32)\n",
    "stddevV=tf.cast(tf.truediv(1,num_filters_c1+1),tf.float32)\n",
    "stddevW=tf.cast(tf.truediv(1,num_hid_units+1),tf.float32)\n",
    "\n",
    "# creating placeholders train_data, train_labels, vali_data, vali_labels\n",
    "# samples are along columns. Add biases (1) to data before use!\n",
    "# Labels are already one-hot encoded \n",
    "# taking input_vec_size+1 to account for bias\n",
    "train_data_pl=tf.placeholder(tf.float32,shape=(None,max_num_words,input_vec_size,1))\n",
    "train_labels_pl=tf.placeholder(tf.int32,shape=(None,num_categ))\n",
    "vali_labels_pl=tf.placeholder(tf.int32,shape=(None,))\n",
    "\n",
    "# Define all variables\n",
    "V=tf.Variable(tf.random_normal([num_hid_units,4*4*64],\n",
    "                               mean=0.0,stddev=stddevV))\n",
    "W=tf.Variable(tf.random_normal([num_categ,num_hid_units],\n",
    "                               mean=0.0,stddev=stddevW))\n",
    "w_bias=tf.Variable(tf.random_normal([num_categ,1],\n",
    "                               mean=0.0,stddev=stddevW))\n",
    "\n",
    "# 'Create' the main graph\n",
    "z_pre_soft,z=cnn_net(train_data_pl,V,W,w_bias)\n",
    "\n",
    "# Predicting\n",
    "pred_labels=tf.argmax(z, axis = 0)\n",
    "\n",
    "# loss\n",
    "loss=tf.nn.softmax_cross_entropy_with_logits(labels = train_labels_pl, logits = tf.transpose(z_pre_soft))\n",
    "# # loss=tf.nn.sigmoid_cross_entropy_with_logits(labels = tf.cast(train_labels_pl, tf.float32), \n",
    "# #                                              logits = tf.transpose(z_pre_soft))\n",
    "\n",
    "# Session\n",
    "sess=tf.InteractiveSession()\n",
    "\n",
    "# Initializing\n",
    "init=tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# optimizing loss\n",
    "optimizer=tf.train.GradientDescentOptimizer(epsw)\n",
    "train_step=optimizer.minimize(loss)\n",
    "\n",
    "minibatch=2\n",
    "\n",
    "# running mini-batch SGD\n",
    "for curr_iter in np.arange(max_iter):\n",
    "    if curr_iter%5000==0:\n",
    "        print('Curr iter is',curr_iter)\n",
    "    \n",
    "    # select appropriate mini-batch\n",
    "    \n",
    "    # generate random i\n",
    "    i=random.randint(0,47999)\n",
    "    if i<=47998:\n",
    "        j=i+1\n",
    "    else:\n",
    "        j=0\n",
    "    train_data_batch=traindata[[i,j],:,:,:]\n",
    "    train_labels_batch=trainlabels[[i,j],:]\n",
    "\n",
    "    sess.run(train_step,feed_dict={train_data_pl: train_data_batch,\n",
    "                                   train_labels_pl: train_labels_batch})\n",
    "#     i+=2\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "vali_labels_pl=tf.placeholder(tf.int32,shape=(None,))\n",
    "vali_size_tf = tf.constant(vali_size)\n",
    "accu = tf.truediv(vali_size_tf - tf.cast(tf.count_nonzero(\n",
    "        tf.cast(pred_labels, tf.int32) - vali_labels_pl), tf.int32), vali_size_tf)\n",
    "\n",
    "pred_labels_shape=tf.shape(pred_labels)\n",
    "pred_labels_nzero=tf.count_nonzero(pred_labels)\n",
    "vali_labels_shape=tf.shape(vali_labels_pl)\n",
    "\n",
    "sess.run(accu, feed_dict = {train_data_pl: validata, vali_labels_pl: valilabels})"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
